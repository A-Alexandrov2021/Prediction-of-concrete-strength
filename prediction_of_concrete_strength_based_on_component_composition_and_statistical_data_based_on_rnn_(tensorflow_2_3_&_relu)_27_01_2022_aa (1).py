# -*- coding: utf-8 -*-
"""Prediction of concrete strength based on component composition and statistical data based on RNN (TensorFlow 2.3 & ReLU)_27_01_2022_AA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SzwKrI3LOvIJaEueNnWJhONRTMRFDAVS
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import pandas as pd
import datetime, os


#logs_base_dir = ".logs"
#os.makedirs(logs_base_dir, exist_ok = True)
#%tensorboard --logdir {logs_base_dir}

"""**Prediction of concrete strength based on composition of composition and statistical data based on RNN + LSTM**"""

import matplotlib.pyplot as plt

# def generate_features(df):
    """
    Generate features for a stock/index based on historical price and performance
    @param df: dataframe with columns "Open", "Close", "High", "Low", "Volume", "Adjusted Close"
    @return: dataframe, data set with new features
    """
    df_new = pd.DataFrame()
    # 6 original features
    #df_new['Date'] = df['Date']
    df_new['open'] = df['open']
    df_new['open_1'] = df['open'].shift(1)
    df_new['close_1'] = df['close'].shift(1)
    df_new['high_1'] = df['high'].shift(1)
    df_new['low_1'] = df['low'].shift(1)
    df_new['volume_1'] = df['volume'].shift(1)
    # 31 generated features
    # average price
    df_new['avg_price_5'] = df['close'].rolling(5).mean().shift(1)
    df_new['avg_price_30'] = df['close'].rolling(21).mean().shift(1)
    df_new['avg_price_365'] = df['close'].rolling(252).mean().shift(1)
    df_new['ratio_avg_price_5_30'] = df_new['avg_price_5'] / df_new['avg_price_30']
    df_new['ratio_avg_price_5_365'] = df_new['avg_price_5'] / df_new['avg_price_365']
    df_new['ratio_avg_price_30_365'] = df_new['avg_price_30'] / df_new['avg_price_365']
    # average volume
    df_new['avg_volume_5'] = df['volume'].rolling(5).mean().shift(1)
    df_new['avg_volume_30'] = df['volume'].rolling(21).mean().shift(1)
    df_new['avg_volume_365'] = df['volume'].rolling(252).mean().shift(1)
    df_new['ratio_avg_volume_5_30'] = df_new['avg_volume_5'] / df_new['avg_volume_30']
    df_new['ratio_avg_volume_5_365'] = df_new['avg_volume_5'] / df_new['avg_volume_365']
    df_new['ratio_avg_volume_30_365'] = df_new['avg_volume_30'] / df_new['avg_volume_365']
    # standard deviation of prices
    df_new['std_price_5'] = df['close'].rolling(5).std().shift(1)
    df_new['std_price_30'] = df['close'].rolling(21).std().shift(1)
    df_new['std_price_365'] = df['close'].rolling(252).std().shift(1)
    df_new['ratio_std_price_5_30'] = df_new['std_price_5'] / df_new['std_price_30']
    df_new['ratio_std_price_5_365'] = df_new['std_price_5'] / df_new['std_price_365']
    df_new['ratio_std_price_30_365'] = df_new['std_price_30'] / df_new['std_price_365']
    # standard deviation of volumes
    df_new['std_volume_5'] = df['volume'].rolling(5).std().shift(1)
    df_new['std_volume_30'] = df['volume'].rolling(21).std().shift(1)
    df_new['std_volume_365'] = df['volume'].rolling(252).std().shift(1)
    df_new['ratio_std_volume_5_30'] = df_new['std_volume_5'] / df_new['std_volume_30']
    df_new['ratio_std_volume_5_365'] = df_new['std_volume_5'] / df_new['std_volume_365']
    df_new['ratio_std_volume_30_365'] = df_new['std_volume_30'] / df_new['std_volume_365']
    # # return
    df_new['return_1'] = ((df['close'] - df['close'].shift(1)) / df['close'].shift(1)).shift(1)
    df_new['return_5'] = ((df['close'] - df['close'].shift(5)) / df['close'].shift(5)).shift(1)
    df_new['return_30'] = ((df['close'] - df['close'].shift(21)) / df['close'].shift(21)).shift(1)
    df_new['return_365'] = ((df['close'] - df['close'].shift(252)) / df['close'].shift(252)).shift(1)
    df_new['moving_avg_5'] = df_new['return_1'].rolling(5).mean().shift(1)
    df_new['moving_avg_30'] = df_new['return_1'].rolling(21).mean().shift(1)
    df_new['moving_avg_365'] = df_new['return_1'].rolling(252).mean().shift(1)
    # the target
    df_new['close'] = df['close']
    df_new = df_new.dropna(axis=0)
    return df_new

#df = pd.read_csv("19880101_20161231.csv", index_col='Date')
#df = pd.read_csv('concrete.csv')


data = generate_features(df)

print(data.head(500))

data = pd.read_csv('concrete.csv')

"""**Prediction of concrete strength based on composition of composition and statistical data based on RNN + LSTM**"""

#print(data)
print(data)

start_train = int(0)
end_train = int(773)

start_test = int(774)
end_test = int(1030)



data_train = data.loc[start_train:end_train]
X_train = data_train.drop('strength', axis=1).values
y_train = data_train['strength'].values


data_test = data.loc[start_test:end_test]
X_test = data_test.drop('strength', axis=1).values
y_test = data_test['strength'].values

y_train

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X_scaled_train = scaler.fit_transform(X_train)
X_scaled_test = scaler.transform(X_test)

#X_scaled_test

import tensorflow as tf
from tensorflow import keras
#from tensorflow.keras import layers

tf.random.set_seed(42)
model = keras.Sequential([
    keras.layers.Dense(units=64, activation='relu'),
    keras.layers.Dense(units=64, activation='relu'),
    #keras.layers.Dense(units=32, activation='relu'),
    #tf.keras.layers.Dropout(0.5),
    keras.layers.Dense(units=1)
])



model = tf.keras.models.load_model('keras_model.h5')

# Check its architecture
model.summary()

model.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(0.07))

model.fit(X_scaled_train, y_train, epochs=100, verbose=True)

predictions1 = model.predict(X_scaled_test)[:, 0]

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
print(f'MSE: {mean_squared_error(y_test, predictions1):.3f}')
print(f'MAE: {mean_absolute_error(y_test, predictions1):.3f}')
print(f'R^2: {r2_score(y_test, predictions1):.3f}')

from tensorboard.plugins.hparams import api as hp
HP_HIDDEN = hp.HParam('hidden_size', hp.Discrete([64, 32, 16]))
HP_EPOCHS = hp.HParam('epochs', hp.Discrete([300, 1000]))
HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(0.01, 0.4))

def train_test_model(hparams, logdir):
    model = keras.Sequential([
       keras.layers.Dense(units=hparams[HP_HIDDEN], activation='relu'),
       tf.keras.layers.Dropout(0.5),
       keras.layers.Dense(units=1)
    ])
    model.compile(loss='mean_squared_error',
                  optimizer=tf.keras.optimizers.Adam(hparams[HP_LEARNING_RATE]),
                  metrics=['mean_squared_error'])
    model.fit(X_scaled_train, y_train, validation_data=(X_scaled_test, y_test), epochs=hparams[HP_EPOCHS], verbose=False,
              callbacks=[
                  tf.keras.callbacks.TensorBoard(logdir),  # log metrics
                  hp.KerasCallback(logdir, hparams),  # log hparams
                  tf.keras.callbacks.EarlyStopping(
                      monitor='val_loss', min_delta=0, patience=200, verbose=0, mode='auto',
                  )
              ],
              )
    _, mse = model.evaluate(X_scaled_test, y_test)
    pred = model.predict(X_scaled_test)
    r2 = r2_score(y_test, pred)
    return mse, r2

def run(hparams, logdir):
    with tf.summary.create_file_writer(logdir).as_default():
        hp.hparams_config(
            hparams=[HP_HIDDEN, HP_EPOCHS, HP_LEARNING_RATE],
            metrics=[hp.Metric('mean_squared_error', display_name='mse'),
                     hp.Metric('r2', display_name='r2')],
        )
        mse, r2 = train_test_model(hparams, logdir)
        tf.summary.scalar('mean_squared_error', mse, step=1)
        tf.summary.scalar('r2', r2, step=1)


session_num = 0

for hidden in HP_HIDDEN.domain.values:
    for epochs in HP_EPOCHS.domain.values:
        for learning_rate in tf.linspace(HP_LEARNING_RATE.domain.min_value, HP_LEARNING_RATE.domain.max_value, 5):
            hparams = {
                HP_HIDDEN: hidden,
                HP_EPOCHS: epochs,
                HP_LEARNING_RATE: float("%.2f"%float(learning_rate)),
            }
            run_name = "run-%d" % session_num
            print('--- Starting trial: %s' % run_name)
            print({h.name: hparams[h] for h in hparams})
            run(hparams, 'logs/hparam_tuning/' + run_name)
            session_num += 1



model = keras.Sequential([
    keras.layers.Dense(units=64, activation='relu'),
    keras.layers.Dense(units=64, activation='relu'),
    #tf.keras.layers.Dropout(0.1),
    #keras.layers.Dense(units=16, activation='relu'),
    #keras.layers.Dense(units=32, activation='relu'),
    keras.layers.Dense(units=1)
])

model.compile(loss='mean_squared_error',
              optimizer=tf.keras.optimizers.Adam(0.08))

model.fit(X_scaled_train, y_train, epochs=1000, verbose=False)

#

predictions = model.predict(X_scaled_test)[:, 0]

model.save('data_concrete/keras_model.h5')

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
print(f'MSE: {mean_squared_error(y_test, predictions):.3f}')
print(f'MAE: {mean_absolute_error(y_test, predictions):.3f}')
print(f'R^2: {r2_score(y_test, predictions):.3f}')

plt.plot(data_test.index, y_test, c='k')
plt.plot(data_test.index, predictions, c='b')
plt.plot(data_test.index, predictions, c='r')
plt.plot(data_test.index, predictions, c='g')
#plt.xticks(range(0, 252, 10), rotation=1)
plt.xlabel('Date')
plt.ylabel('Close price')
plt.legend(['Truth', 'Neural network prediction'])
plt.show()



#X_scaled_train

print(predictions)

y_test